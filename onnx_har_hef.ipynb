{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcory-hub/hailo-colab/blob/main/onnx_har_hef.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1hxKMSZkP_3"
      },
      "source": [
        "# From onnx via har to hef\n",
        "\n",
        "Goal of this notebook is to make a `HEF` file is the file that runs on the `hailo-8l` device that is on the AI-kit. For a schematic overview and more details check the hailo docs about the [model build process](https://hailo.ai/developer-zone/documentation/dataflow-compiler-v3-29-0/?sp_referrer=overview/overview.html).\n",
        "\n",
        "Credits to trieut415! A lot of the code from hailo is adjusted inspired on his [post](https://community.hailo.ai/t/guide-to-using-the-dfc-to-convert-a-modified-yolov11-on-google-colab/7131/3) in the hailo community. Especially the solution to run the Dataflow Compiler in a virtual environment solved my initial problem. Furthermore, the codeblock to make the calibration data is more robust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kVVRWr9TprW"
      },
      "source": [
        "## Overview\n",
        "### 1. Parsing from onnx to har:\n",
        "- Input: onnx file\n",
        "- Output: har file (model representation and parameters (32-bits weights))\n",
        "\n",
        "### 2. Model Optimization:\n",
        "- input: har file (32-bits) and calibration images\n",
        "- output: har file (optimized model representation and parameters (quantized weights))\n",
        "\n",
        "  Conversion of the har file with float32 parameters to integers. To convert te parameters tun the model emulation in native mode on a small set of images (not annotated).\n",
        "\n",
        "  #### Substeps\n",
        "\n",
        "  1. Prepare callibration set\n",
        "  2. Load har file (32-bits) from model conversion\n",
        "  3. Create model script\n",
        "\n",
        "\n",
        "### 3. Model Compilation:\n",
        "- input: har (optimized)\n",
        "- output: hef\n",
        "\n",
        "  The quantized model is compiled into a specific binary format called HEF (Hailo Executable Format). This format is optimized for the Hailo device's architecture and allows for efficient execution of the model's operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJWdt-dboiFw"
      },
      "source": [
        "## Before your start\n",
        "1. Make a folder 'hailo' in your Google Drive.\n",
        "2. Download hailo dataflow compiler  from https://hailo.ai/developer-zone/software-downloads/ (you need to make an account) and upload it to your Google Drive. To check the python version of Colab you can run the command below.\n",
        "3. Collect a set of (at least) 1024 images needed for callibration. These images need no annotation, but should be representatieve. Zip it preferably with the name calibrationDataset.zip (On mac use `ditto -c -k --norsrc --keepParent calibrationDataset calibrationDataset.zip`)\n",
        "4. Spin up a Colab with GPU, needed for the optimization step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs5P5fxEpbnb"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iAAzfrJajBn"
      },
      "source": [
        "## Install Dataflow Compiler (DFC) in virtual environment (venv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwsOA_V3I4zW"
      },
      "source": [
        "For the next codeblock, make sure you downloaded hailo dataflow compiler (python 3.10) from https://hailo.ai/developer-zone/software-downloads/ and copied the .whl to your Google Drive. Change the filename if their was an update.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUPnZCbI96Al"
      },
      "outputs": [],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Make virual environment\n",
        "\n",
        "# Update and install packages needed for DFC\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-dev python3-distutils python3-tk libfuse2 graphviz libgraphviz-dev\n",
        "\n",
        "# Will need a venv to install the DFC in\n",
        "!pip install --upgrade pip virtualenv\n",
        "!virtualenv my_env\n",
        "\n",
        "#Installing the Dataflow Compiler, update the filename if needed\n",
        "!my_env/bin/pip install /content/gdrive/MyDrive/hailo/hailo_dataflow_compiler-3.29.0-py3-none-linux_x86_64.whl\n",
        "\n",
        "# Check the version and show help information\n",
        "!my_env/bin/hailo --version\n",
        "#!my_env/bin/hailo -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euWouzZiKcEm"
      },
      "source": [
        "## 1.  Parsing from onnx --> har\n",
        "\n",
        "1. Select the hardware architecture. For the raspberry AI-kits it is `hailo8l`.\n",
        "2. Open the [netron](https://netron.app/) site, Click `Open Model` and select your onnx file on your local computer.\n",
        "3. To identify the end nodes, they are the nodes right before the post-processing operations at the very bottom of the model. Their are 2 end nodes per map. I used a search for `onnx::Reshape` to get to the two `conv` layers that pointed to the `onnx::Reshape`.\n",
        "\n",
        "  In an unmodified yolo11 model this are the endpoints:\n",
        "  ```\n",
        "\"/model.23/cv2.2/cv2.2.2/Conv\",\n",
        "\"/model.23/cv3.2/cv3.2.2/Conv\",\n",
        "\"/model.23/cv2.1/cv2.1.2/Conv\",\n",
        "\"/model.23/cv3.1/cv3.1.2/Conv\",\n",
        "\"/model.23/cv2.0/cv2.0.2/Conv\",\n",
        "\"/model.23/cv3.0/cv3.0.2/Conv\",\n",
        "```\n",
        "  If they are different, then depicted above you have to change it in the code block below.\n",
        "4. Check the net_input_shapes in netron. Adjust it if your \"input layer name\": [batch, rgb, image size] are different from:\n",
        "  ```\n",
        "  \"images\": [1, 3, 640, 640]\n",
        "  ```\n",
        "\n",
        "5. Run the codeblocks below. The har file is created by the command `runner.translate_onnx_model` and saved with `runner.save_har`. To use the DFC in the venv we make and save the python code in the first codeblock and run it in the venv in the second codeblock. More details about the conversion can be found in the [Parsing tutorial](https://hailo.ai/developer-zone/documentation/dataflow-compiler-v3-29-0/?sp_referrer=tutorials_notebooks/notebooks/DFC_1_Parsing_Tutorial.html).\n",
        "\n",
        "(Note: onnx_model_name is without .onnx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_VrxC1pIsv3"
      },
      "outputs": [],
      "source": [
        "with open(\"translate_model.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "\n",
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Set hailo hardware architecture and onnx model and model path\n",
        "chosen_hw_arch = \"hailo8l\" # @param [\"hailo8l\", \"hailo8\", \"hailo8r\", \"hailo10h\", \"hailo15h\", \"hailo15m\"]\n",
        "onnx_model_name = \"best\" # @param {type:\"string\"}\n",
        "onnx_path = \"/content/gdrive/MyDrive/hailo/best.onnx\" # @param {type:\"string\"}\n",
        "\n",
        "print(\"Starting model translation...\")\n",
        "\n",
        "# Initialize the ClientRunner\n",
        "runner = ClientRunner(hw_arch=chosen_hw_arch)\n",
        "\n",
        "# Change the end_node_names if netron show different end nodes\n",
        "end_node_names = [\n",
        "  \"/model.22/cv2.0/cv2.0.2/Conv\",\n",
        "  \"/model.22/cv3.0/cv3.0.2/Conv\",\n",
        "  \"/model.22/cv2.1/cv2.1.2/Conv\",\n",
        "  \"/model.22/cv3.1/cv3.1.2/Conv\",\n",
        "  \"/model.22/cv2.2/cv2.2.2/Conv\",\n",
        "  \"/model.22/cv3.2/cv3.2.2/Conv\",\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Translate the onnx model to har file\n",
        "    hn, npz = runner.translate_onnx_model(\n",
        "        onnx_path,\n",
        "        onnx_model_name,\n",
        "        end_node_names=end_node_names,\n",
        "        net_input_shapes={\"images\": [1, 3, 640, 640]},  # Adjust input shapes if needed\n",
        "    )\n",
        "    print(\"Model translation successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during model translation: {e}\")\n",
        "    raise\n",
        "\n",
        "# Save the har file\n",
        "hailo_model_har_name = f\"{onnx_model_name}_hailo_model.har\"\n",
        "try:\n",
        "    runner.save_har(hailo_model_har_name)\n",
        "    print(f\"HAR file saved as: {hailo_model_har_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving HAR file: {e}\")\n",
        "\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model in CLI\n",
        "!my_env/bin/python translate_model.py"
      ],
      "metadata": {
        "id": "3HrHU4wF6kp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Save hailo_model.har to google drive."
      ],
      "metadata": {
        "id": "tgvxG933CinG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "!cp /content/best_hailo_model.har /content/gdrive/MyDrive/hailo/best_hailo_model.har"
      ],
      "metadata": {
        "id": "oYXP9cyyCSCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR5d4kdrLlJz"
      },
      "source": [
        "## 2. Model optimization\n",
        "The optimization from Hailo replaced by the optimization in the guide from trieut415.\n",
        "\n",
        "1. Print dictionary of layers and operations\n",
        "2. Load har\n",
        "3. create model script\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVkx9ZdKwmkF"
      },
      "source": [
        "1. Print layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyfvCJe7x3wF"
      },
      "outputs": [],
      "source": [
        "with open(\"inspect_layers.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "\n",
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Load the HAR file\n",
        "har_path = \"/content/best_hailo_model.har\" # @param {type:\"string\"}\n",
        "\n",
        "runner = ClientRunner(har=har_path)\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "try:\n",
        "    # Access the HailoNet as an OrderedDict\n",
        "    hn_dict = runner.get_hn()  # Or use runner._hn if get_hn() is unavailable\n",
        "    print(\"Inspecting layers from HailoNet (OrderedDict):\")\n",
        "\n",
        "    # Pretty-print each layer\n",
        "    for key, value in hn_dict.items():\n",
        "        print(f\"Key: {key}\")\n",
        "        pprint(value)\n",
        "        print(\"\\\\n\" + \"=\"*80 + \"\\\\n\")  # Add a separator between layers for clarity\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error while inspecting hn_dict: {e}\")\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Iktg68-3Dmj"
      },
      "outputs": [],
      "source": [
        "# Run model in CLI\n",
        "!my_env/bin/python inspect_layers.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITlcUUSC7K9W"
      },
      "source": [
        "On the top of the output the output_layers_order is printed. It should look like this. The renamed layers we need to check in de codeblock below and adjust if needed.\n",
        "\n",
        "```\n",
        "Inspecting layers from HailoNet (OrderedDict):\n",
        "Key: name\n",
        "'best'\n",
        "\n",
        "================================================================================\n",
        "\n",
        "Key: net_params\n",
        "OrderedDict([('version', '1.0'),\n",
        "             ('stage', 'HN'),\n",
        "             ('clusters_placement', [[]]),\n",
        "             ('clusters_to_skip', []),\n",
        "             ('output_layers_order',\n",
        "              ['best/conv41',\n",
        "               'best/conv42',\n",
        "               'best/conv52',\n",
        "               'best/conv53',\n",
        "               'best/conv62',\n",
        "               'best/conv63']),\n",
        "             ('transposed_net', False),\n",
        "             ('net_scopes', ['best'])])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj2BYp9cDGxA"
      },
      "source": [
        "Check it the output layers have the correct name in the code in the next codeblock. Adjust if needed. [yolov8s](https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/ObjectDetection/Detection-COCO/yolo/yolov8s/2023-02-02/yolov8s.zip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKSdmDLf8RUb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "# Updated NMS layer configuration dictionary\n",
        "nms_layer_config = {\n",
        "    \"nms_scores_th\": 0.001,\n",
        "    \"nms_iou_th\": 0.7,\n",
        "    \"image_dims\": [\n",
        "        640,\n",
        "        640\n",
        "    ],\n",
        "    \"max_proposals_per_class\": 100,\n",
        "    \"classes\": 5,\n",
        "    \"regression_length\": 16,\n",
        "    \"background_removal\": False,\n",
        "    \"background_removal_index\": 0,\n",
        "    \"bbox_decoders\": [\n",
        "        {\n",
        "            \"name\": \"best/bbox_decoder41\",\n",
        "            \"stride\": 8,\n",
        "            \"reg_layer\": \"best/conv41\",\n",
        "            \"cls_layer\": \"best/conv42\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"best/bbox_decoder52\",\n",
        "            \"stride\": 16,\n",
        "            \"reg_layer\": \"best/conv52\",\n",
        "            \"cls_layer\": \"best/conv53\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"best/bbox_decoder62\",\n",
        "            \"stride\": 32,\n",
        "            \"reg_layer\": \"best/conv62\",\n",
        "            \"cls_layer\": \"best/conv63\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Path to save the updated JSON configuration\n",
        "output_dir = \"/content/\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "output_path = os.path.join(output_dir, \"nms_layer_config.json\")\n",
        "\n",
        "# Save the updated configuration as a JSON file\n",
        "with open(output_path, \"w\") as json_file:\n",
        "    json.dump(nms_layer_config, json_file, indent=4)\n",
        "\n",
        "print(f\"NMS layer configuration saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Save nms_layer_config.json to google drive."
      ],
      "metadata": {
        "id": "1xcKlrqVDxuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "!cp /content/nms_layer_config.json /content/gdrive/MyDrive/hailo/nms_layer_config.json"
      ],
      "metadata": {
        "id": "pXp5aibnDpFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcgJDdLZMEby"
      },
      "source": [
        "### 2.1 Calibration data\n",
        "\n",
        "\n",
        "- The dataset should contain at least 1024 representative images (not labeled).\n",
        "- Use a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuHkgWfYP50Y"
      },
      "source": [
        "1. Unzip the calibration dataset and rename the folder to `calibrationDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F29cF26uNl2L"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define Paths with Parameters\n",
        "calibrationset_path = \"/content/gdrive/MyDrive/hailo/calibrationDataset.zip\" # @param {type:\"string\"}\n",
        "calibrationset_filename = \"calibrationDataset\" # @param {type:\"string\"}\n",
        "\n",
        "try:\n",
        "  # Unzip the Dataset\n",
        "  !unzip {calibrationset_path} -d '/content/'\n",
        "\n",
        "  # Rename the Extracted Folder\n",
        "  old_path = f'/content/{calibrationset_filename}'\n",
        "  new_path = '/content/calibrationDataset'\n",
        "  if os.path.exists(old_path):\n",
        "    os.rename(old_path, new_path)\n",
        "  else:\n",
        "    print(f\"Error: {old_path} does not exist.\")\n",
        "except Exception as e:\n",
        "  print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGIYnr5PQOs1"
      },
      "source": [
        "2. Make calibration data. Adjust the size of the image if you input layer has an other format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifbGGXLdMFsB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Paths to directories and files\n",
        "image_dir = '/content/calibrationDataset'\n",
        "output_dir = '/content/'\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# File paths for saving calibration data\n",
        "calibration_data_path = os.path.join(output_dir, \"calibration_data.npy\")\n",
        "processed_data_path = os.path.join(output_dir, \"processed_calibration_data.npy\")\n",
        "\n",
        "# Initialize an empty list for calibration data\n",
        "calib_data = []\n",
        "\n",
        "# Process all image files in the directory\n",
        "for img_name in os.listdir(image_dir):\n",
        "    img_path = os.path.join(image_dir, img_name)\n",
        "    if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        img = Image.open(img_path).resize((640, 640))  # CHANGE desired dimensions\n",
        "        img_array = np.array(img) / 255.0  # Normalize to [0, 1]\n",
        "        calib_data.append(img_array)\n",
        "\n",
        "# Convert the calibration data to a NumPy array\n",
        "calib_data = np.array(calib_data)\n",
        "\n",
        "# Save the normalized calibration data\n",
        "np.save(calibration_data_path, calib_data)\n",
        "print(f\"Normalized calibration dataset saved with shape: {calib_data.shape} to {calibration_data_path}\")\n",
        "\n",
        "# Scale the normalized data back to [0, 255]\n",
        "processed_calibration_data = calib_data * 255.0\n",
        "\n",
        "# Save the processed calibration data\n",
        "np.save(processed_data_path, processed_calibration_data)\n",
        "print(f\"Processed calibration dataset saved with shape: {processed_calibration_data.shape} to {processed_data_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart to clear RAM"
      ],
      "metadata": {
        "id": "KO7JGjzKGcgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Restart the runtime to clear RAM\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "QOrH_56BElZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Zip and save both calibration files to google drive. Time to take a coffee..."
      ],
      "metadata": {
        "id": "wrowyI2dF9Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "!zip -r processedCalibrationData.zip /content/processed_calibration_data.npy\n",
        "\n",
        "!cp /content/processedCalibrationData.zip /content/drive/MyDrive/hailo/"
      ],
      "metadata": {
        "id": "kWp3Abb4AkUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "!zip -r calibrationData.zip  /content/calibration_data.npy\n",
        "\n",
        "!cp /content/calibrationData.zip /content/drive/MyDrive/hailo/"
      ],
      "metadata": {
        "id": "Ht66kIwtVf6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Remove the folder calibrationDataset."
      ],
      "metadata": {
        "id": "ldWfLktdWzUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "try:\n",
        "  shutil.rmtree('calibrationDataset')\n",
        "  print(\"Folder 'calibrationDataset' removed successfully.\")\n",
        "except FileNotFoundError:\n",
        "  print(\"Folder 'calibrationDataset' does not exist.\")\n",
        "except OSError as e:\n",
        "  print(f\"Error removing folder: {e}\")"
      ],
      "metadata": {
        "id": "L7qk_umSaZqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Needed for debugging final step: run first 3 codeblock and then import most recent files that are needed for optimization."
      ],
      "metadata": {
        "id": "xZXKb1HeJ6_8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZceF00q25mWb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Define the list of files directly within the code\n",
        "file_paths = [\n",
        "    '/content/gdrive/MyDrive/hailo/best_hailo_model.har',\n",
        "    '/content/gdrive/MyDrive/hailo/nms_layer_config.json',\n",
        "    '/content/gdrive/MyDrive/hailo/processed_calibration_data.npy',\n",
        "]\n",
        "\n",
        "for file_path in file_paths:\n",
        "  source_path = f'{file_path}'\n",
        "  !cp -r '{source_path}' '.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQ99d8JMMG2"
      },
      "source": [
        "Now, weâ€™re finally ready to optimize it with this script, you can find sample .alls files here, I referenced yolo10nms.json as a base to create my alls file.\n",
        "\n",
        "Note that the change_output_activation applied to my CLS_layer, you can go back and verify this with Netron like specified above.\n",
        "\n",
        "\n",
        "[yolov8s alls](https://github.com/hailo-ai/hailo_model_zoo/blob/master/hailo_model_zoo/cfg/alls/generic/yolov8s.alls)\n",
        "```\n",
        "normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])\n",
        "change_output_activation(conv42, sigmoid)\n",
        "change_output_activation(conv53, sigmoid)\n",
        "change_output_activation(conv63, sigmoid)\n",
        "nms_postprocess(\"../../postprocess_config/yolov8s_nms_config.json\", meta_arch=yolov8, engine=cpu)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zXSH2RlMHj1"
      },
      "outputs": [],
      "source": [
        "with open(\"optimize_model.py\", \"w\") as f:\n",
        "\n",
        "    f.write(\"\"\"\n",
        "\n",
        "import os\n",
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Define your model's HAR file name\n",
        "model_name = \"best\"\n",
        "hailo_model_har_name = f\"{model_name}_hailo_model.har\"\n",
        "\n",
        "\n",
        "# Ensure the HAR file exists\n",
        "assert os.path.isfile(f\"{model_name}_hailo_model.har\")\n",
        "\n",
        "# Initialize the ClientRunner with the HAR file\n",
        "runner = ClientRunner(har=hailo_model_har_name)\n",
        "\n",
        "# Define the model script to add a normalization layer\n",
        "# Normalization for [0, 1] range\n",
        "alls = \\\"\\\"\\\"\n",
        "normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])\n",
        "change_output_activation(conv42, sigmoid)\n",
        "change_output_activation(conv53, sigmoid)\n",
        "change_output_activation(conv63, sigmoid)\n",
        "nms_postprocess(\"/content/nms_layer_config.json\", meta_arch=yolov8, engine = cpu)\n",
        "# Commented out this line below (not in https://github.com/hailo-ai/hailo_model_zoo/blob/master/hailo_model_zoo/cfg/alls/generic/yolov8s.alls)\n",
        "# performance_param(compiler_optimization_level=max)\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "# Load the model script into the ClientRunner\n",
        "runner.load_model_script(alls)\n",
        "\n",
        "# Define a calibration dataset\n",
        "# Replace 'calib_dataset' with the actual dataset you're using for calibration\n",
        "# For example, if it's a directory of images, prepare the dataset accordingly\n",
        "calib_dataset = \"/content/processed_calibration_data.npy\"\n",
        "\n",
        "# Perform optimization with the calibration dataset\n",
        "runner.optimize(calib_dataset)\n",
        "\n",
        "# Save the optimized model to a new Quantized HAR file\n",
        "quantized_model_har_path = f\"{model_name}_quantized_model.har\"\n",
        "runner.save_har(quantized_model_har_path)\n",
        "\n",
        "print(f\"Quantized HAR file saved to: {quantized_model_har_path}\")\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfq92L58MayG"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/python optimize_model.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Copy best quantized model.har to google drive."
      ],
      "metadata": {
        "id": "0GBFn5h5cs4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/best_quantized_model.har /content/drive/MyDrive/hailo/best_quantized_model.har"
      ],
      "metadata": {
        "id": "vT1Xkm33dJ74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqvg9ACHMg6m"
      },
      "source": [
        "#3. Model compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay75npCQMjZg"
      },
      "outputs": [],
      "source": [
        "with open(\"compile_model.py\", \"w\") as f:\n",
        "\n",
        "    f.write(\"\"\"\n",
        "from hailo_sdk_client import ClientRunner\n",
        "\n",
        "# Define the quantized model HAR file\n",
        "model_name = \"best\"\n",
        "quantized_model_har_path = f\"/content/best_quantized_model.har\"\n",
        "\n",
        "# Initialize the ClientRunner with the HAR file\n",
        "runner = ClientRunner(har=quantized_model_har_path)\n",
        "print(\"[info] ClientRunner initialized successfully.\")\n",
        "\n",
        "# Compile the model\n",
        "try:\n",
        "    hef = runner.compile()\n",
        "    print(\"[info] Compilation completed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"[error] Failed to compile the model: {e}\")\n",
        "    raise\n",
        "file_name = f\"{model_name}.hef\"\n",
        "with open(file_name, \"wb\") as f:\n",
        "    f.write(hef)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKKPT3ayM-r-"
      },
      "outputs": [],
      "source": [
        "!my_env/bin/python compile_model.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lqfr0q9oPVUSAS2xvsBKIRXgTIBrTFgK",
      "authorship_tag": "ABX9TyPDriTaqdMBpYlU1p9PtNwf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}